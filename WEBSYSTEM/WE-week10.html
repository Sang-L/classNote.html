<! docytype html>



<html>
<head>
<h1>웹시스템 10주차 강의 노트</h1>
<h3><a href="../week-10.html">back</a></h3>
</head>

<body>
신경망<br><br><br>


-뉴련 : 다른 신경세포에서 오는 신호를 받아 자신의 신호를 내보내는 역할을 수행<br>
인간의 뇌를 구성하는 요소<br><br>

-뇌 : 신경세포들이 연결된 거대한 네트워크 -> 신경망, 500조개의 시냅스로 구성됨 ,860억개의 뉴런으로 이루어짐<br><br>


1. 인공 신경망<br><br>

-1)인공 신경망<br>
-인공 뉴런 : 선형 구조와 비선형 변환의 결합<br>
--선형 구조 : 입력 데이터와 가중치<br>
--비선형 변환 : 활성 함수<br>
-여러 인공 뉴런들로 구성된 네트워크<br>
-가장 복잡한 논리 방정식 계산이 가능함.<br><br>

-2)인공 신경망 구성<br>
-노드<br>
-가중치를 가지는 간선<br>
-활성 함수<br>
-손실 함수<br><br>

인공신경망은?<br>
-1)뉴련의 수학적 모델에서 출발<br>
(뉴런 : 뇌의 핵심 구조로 학습을 가능하게 함)<br>
-2)뇌와 인공신경망의 비교<br>
-신경세포 : 노드<br>
-신경세포들의 연결 : 노드들의 연결 가중치<br>
-신경세포의 신호 : 활성함수<br>
-3)데이터 구조 측면에서 본 인공 신경망<br>
-방향 그래프<br>
-가중치 그래프<br>
-비순환 그래프<br>
--> 가중치를 가진 방향 비순환 그래프<br><br>

-4)인공 신경망의 표현(그래프) (그림<br>
-5)인공 신경망 모형 (그림<br><br>

-6)신경망의 구성<br>
-노드와 간선으로 구성<br>
-노드 : 각 층을 구성하는 요소<br>
-간선 : 노드 간의 가중치<br>
-층의 종류<br>
--입력층<br>
--은닉층<br>
--출력층<br>
-활성함수 : 은닉층 및 출력층<br>
-손실함수 : 출력층<br><br>

-7)활성함수<br>
: 입력 신호의 합을 출력 신호로 변환하는 함수<br>
: 입력 신호의 합을 사요하여 활성화 여부를 결정<br>
-단순 퍼셉트론에서 활성함수 : Heaviside step function<br>
그림)
-활성함수 선정<br>
: 반드시 비선형 함수를 사용<br>
: 선형 함수를 사용하면 신경망의 층 깊이의 의미가 없어짐<br>
: 즉, 은닉층이 없는 네트워크가 됨<br>
-은닉층의 활성함수<br>
: Sigmoid function<br>
: Tanh function<br>
: ReLU(Rectified Linear Unit)<br>
-출력층의 활성함수<br>
:회귀 : 항등함수<br>
: 2 class 분류 : sigmoid함수(아웃풋이 두개)<br>
: Multi-class 분류 : softmax함수(아웃풋이 여러개)<br>
그림많음<br><br>


-8)손실함수<br>
-신경망으로 학습한 결과와 실제 값의 유사도(혹은 오차)를 양적으로 표현하는 수단<br>
-손실함수를 최소로 하는 매개변수(가중치)를 구하는 과정인 학습의 기준<br>
-예) 학습 데이터에 대한 오차의 평균을 사용<br>
-손실함수의 유형 : 회귀, 분류, 재구성<br>
-평균제곱오차(MSE)<br>
--based on the difference between desired and predicted<br>
--평균을 최적화하는 것과 동일<br>
--Outliers에 민감<br>
그림<br><br>

--1)평균제곱오차<br>
-회귀에 주로 사용<br>
-차원축소에도 사용<br>
-Outliers에 민감 <- output이 크면 그 큰값에 의해 outliers값에도 큰 변화가 있따. <br>
그림<br><br>



-9)Cross entropy<br>
-ex)이미지 분류<br>
-이미지 종류 : 개, 고양이, 호랑이<br>
-분류모델목표 : 이미지를 정확히 분류<br>
-예)목표 : y = (0.0, 1.0, 0.0), 출력값 : y^ = (0.3,0.5,0.2) <br>
-손실함수 : y와 y^의 차이를 표혆<br>
-어떻게? Cross entropy<br>
그림<br>
그림<br><br>

-10)가중치 최적화<br>
-Backpropagation<br>
-Algorthm for estimating weights<br>
-Use chain rule<br>
그림<br><br>

-11)Gradient descent<br>
언덕오르기방법<br>
-점진적으로 최적해를 찾아 가는 방법<br>
-유사한 방법을 반복적으로 수행<br>
-최적화 대상 함수가 conex인 경우, 경사하강법은 최적치를 항상 찾아줌<br>
-지역탐색(local search)<br>
-국소 최적해(local optimal solution)에 빠질 가능성<br><br>

</body>
</html>
